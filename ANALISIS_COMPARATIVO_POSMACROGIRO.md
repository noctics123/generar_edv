# ANÃLISIS COMPARATIVO DETALLADO
## HM_MATRIZTRANSACCIONPOSMACROGIRO.py (DDV) vs HM_MATRIZTRANSACCIONPOSMACROGIRO_EDV.py

---

## ğŸ“Š RESUMEN EJECUTIVO

| Aspecto | DDV Version | EDV Version |
|---------|-------------|-------------|
| **LÃ­neas de cÃ³digo** | 2,554 | 2,608 (+54 lÃ­neas) |
| **Formato** | Databricks notebook | Databricks notebook + optimizaciones |
| **Environment** | DDV (Desarrollo) | EDV (ProducciÃ³n ExploraciÃ³n) |
| **OptimizaciÃ³n** | BÃ¡sica | Avanzada (Spark AQE) |
| **Fecha default** | 2024-07-06 | 2025-09-01 |

---

## ğŸ” DIFERENCIAS CRÃTICAS

### 1. CONFIGURACIÃ“N DE AMBIENTE

#### **DDV Version (Original)**
```python
# Container
CONS_CONTAINER_NAME = "abfss://lhcldata@"

# Storage Account
PRM_STORAGE_ACCOUNT_DDV = 'adlscu1lhclbackd03'

# Schema
PRM_ESQUEMA_TABLA_DDV = 'bcp_ddv_matrizvariables'

# Catalog
PRM_CATALOG_NAME = 'catalog_lhcl_desa_bcp'

# Rutas
PRM_RUTA_ADLS_TABLES = 'desa/bcp/ddv/analytics/matrizvariables'
```

#### **EDV Version (Optimizada)**
```python
# Container
CONS_CONTAINER_NAME = "abfss://bcp-edv-trdata-012@"

# Storage Account
PRM_STORAGE_ACCOUNT_DDV = 'adlscu1lhclbackp05'

# Schema DDV con VIEWS
PRM_ESQUEMA_TABLA_DDV = 'bcp_ddv_matrizvariables_v'  # â† NOTA: _v suffix

# Catalog
PRM_CATALOG_NAME = 'catalog_lhcl_prod_bcp'

# Rutas
PRM_RUTA_ADLS_TABLES = 'data/RUBEN/DEUDA_TECNICA/matrizvariables'

# NUEVOS PARÃMETROS EDV
PRM_CATALOG_NAME_EDV = 'catalog_lhcl_prod_bcp_expl'
PRM_ESQUEMA_TABLA_EDV = 'bcp_edv_trdata_012'
```

**ğŸ”‘ Diferencia Clave:**
- DDV: Lee y escribe en **mismo esquema** (`bcp_ddv_matrizvariables`)
- EDV: Lee de **views DDV** (`bcp_ddv_matrizvariables_v`) y escribe en **esquema EDV** (`bcp_edv_trdata_012`)

---

### 2. ARQUITECTURA DE LECTURA/ESCRITURA

#### **DDV Version**
```python
# Una sola variable para lectura y escritura
PRM_ESQUEMA_TABLA = PRM_CATALOG_NAME + "." + PRM_ESQUEMA_TABLA_DDV
VAL_DESTINO_NAME = PRM_ESQUEMA_TABLA + "." + PRM_TABLE_NAME

# Ejemplo: catalog_lhcl_desa_bcp.bcp_ddv_matrizvariables
```

#### **EDV Version (Arquitectura Separada)**
```python
# LECTURA: Desde views DDV
PRM_ESQUEMA_TABLA = PRM_CATALOG_NAME + "." + PRM_ESQUEMA_TABLA_DDV
# Ejemplo: catalog_lhcl_prod_bcp.bcp_ddv_matrizvariables_v

# ESCRITURA: A esquema EDV
PRM_ESQUEMA_TABLA_ESCRITURA = PRM_CATALOG_NAME_EDV + "." + PRM_ESQUEMA_TABLA_EDV
VAL_DESTINO_NAME = PRM_ESQUEMA_TABLA_ESCRITURA + "." + PRM_TABLE_NAME
# Ejemplo: catalog_lhcl_prod_bcp_expl.bcp_edv_trdata_012.HM_MATRIZTRANSACCIONPOSMACROGIRO_RUBEN_2
```

**ğŸ“Œ Ventajas de EDV:**
- âœ… SeparaciÃ³n de ambientes (lectura desde DDV views, escritura a EDV)
- âœ… No contamina esquema DDV de desarrollo
- âœ… Permite rollback fÃ¡cil (datos fuente intactos)
- âœ… Trazabilidad y auditorÃ­a mejorada

---

### 3. OPTIMIZACIONES SPARK

#### **DDV Version: ConfiguraciÃ³n BÃ¡sica**
```python
spark.conf.set("spark.sql.shuffle.partitions", "1000")
spark.conf.set("spark.sql.decimalOperations.allowPrecisionLoss", False)
spark.conf.set("spark.databricks.io.cache.enabled", False)
```

#### **EDV Version: ConfiguraciÃ³n Avanzada (AQE)**
```python
# ConfiguraciÃ³n bÃ¡sica (igual que DDV)
spark.conf.set("spark.sql.shuffle.partitions", "1000")
spark.conf.set("spark.sql.decimalOperations.allowPrecisionLoss", False)
spark.conf.set("spark.databricks.io.cache.enabled", False)

# NUEVAS OPTIMIZACIONES

# 1. Adaptive Query Execution (AQE)
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
spark.conf.set("spark.sql.adaptive.shuffle.targetPostShuffleInputSize", "64m")

# 2. Particiones optimizadas para cluster mediano
spark.conf.set("spark.sql.shuffle.partitions", "200")  # â† Reducido de 1000 a 200

# 3. Broadcast joins automÃ¡tico
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", str(64*1024*1024))  # 64MB

# 4. Overwrite dinÃ¡mico de particiones
spark.conf.set("spark.sql.sources.partitionOverwriteMode", "dynamic")

# 5. Delta Lake - Auto-optimizaciÃ³n
spark.conf.set("spark.databricks.delta.optimizeWrite.enabled", "true")
spark.conf.set("spark.databricks.delta.autoCompact.enabled", "true")
```

**âš¡ Impacto de Optimizaciones:**

| ConfiguraciÃ³n | DDV | EDV | Beneficio |
|---------------|-----|-----|-----------|
| `spark.sql.shuffle.partitions` | 1000 | 200 | -80% overhead de particiones pequeÃ±as |
| Adaptive Query Execution | âŒ | âœ… | Ajuste dinÃ¡mico de plan de ejecuciÃ³n |
| Skew Join Handling | âŒ | âœ… | Manejo automÃ¡tico de datos desbalanceados |
| Auto Broadcast | âŒ | âœ… | Joins mÃ¡s rÃ¡pidos para dimensiones pequeÃ±as |
| Delta Auto-Compaction | âŒ | âœ… | Menos archivos pequeÃ±os, lecturas mÃ¡s rÃ¡pidas |

---

### 4. OPTIMIZACIONES DE PROCESAMIENTO

#### **OPTIMIZACIÃ“N 1: MaterializaciÃ³n de DataFrames**

**DDV Version (I/O a disco)**
```python
# FunciÃ³n: agruparInformacionMesAnalisis()

# Escribe a disco (costoso en I/O)
dfInfo12Meses.write.format(funciones.CONS_FORMATO_DE_ESCRITURA_EN_DISCO) \
    .mode("overwrite") \
    .save(PRM_CARPETA_RAIZ_DE_PROYECTO + "/temp/" + carpetaTemp)

# Lee desde disco (costoso en I/O)
dfInfo12Meses = spark.read.format(funciones.CONS_FORMATO_DE_ESCRITURA_EN_DISCO) \
    .load(PRM_CARPETA_RAIZ_DE_PROYECTO + "/temp/" + carpetaTemp)
```

**EDV Version (Cache en memoria)**
```python
# FunciÃ³n: agruparInformacionMesAnalisis()

# Cache en memoria (mucho mÃ¡s rÃ¡pido)
dfInfo12Meses = dfInfo12Meses.cache()
dfInfo12Meses.count()  # Materializa el DataFrame en memoria
```

**ğŸ“Š Mejora estimada:**
- EliminaciÃ³n de write/read a disco: **~60-80% mÃ¡s rÃ¡pido**
- Memoria vs Disco: **100-1000x mÃ¡s rÃ¡pido**

---

#### **OPTIMIZACIÃ“N 2: Storage Level**

**DDV Version**
```python
dfMatrizVarTransaccionPosMacrogiro = dfMatrizVarTransaccionPosMacrogiro \
    .persist(StorageLevel.MEMORY_ONLY_2)
```

**EDV Version**
```python
# MEMORY_AND_DISK_2 en vez de MEMORY_ONLY_2
# Si no cabe en memoria, usa disco (evita recomputaciÃ³n costosa)
dfMatrizVarTransaccionPosMacrogiro = dfMatrizVarTransaccionPosMacrogiro \
    .persist(StorageLevel.MEMORY_AND_DISK_2)

# MaterializaciÃ³n explÃ­cita
dfMatrizVarTransaccionPosMacrogiro.count()
```

**ğŸ” ExplicaciÃ³n:**
- `MEMORY_ONLY_2`: Replica 2 veces en memoria. Si no cabe, recomputa (muy costoso)
- `MEMORY_AND_DISK_2`: Replica 2 veces, usa disco si no cabe (mÃ¡s resiliente)

---

#### **OPTIMIZACIÃ“N 3: ConsolidaciÃ³n de Loops**

**DDV Version (3 loops separados)**
```python
# LOOP 1: Procesar MONTO
original_cols = dfMatrizVarTransaccionPosMacrogiroMont.columns
for colName in col_names_monto:
    original_cols.extend([
        func.round(col(colName + "_PRM_U6M")/col(colName + "_PRM_U12M"), 4).alias(colName + "_CRECIMIENTO_6M"),
        func.round(col(colName + "_PRM_U3M")/col(colName + "_PRM_U12M"), 4).alias(colName + "_CRECIMIENTO_3M"),
        func.round(col(colName + "_U1M")/col(colName + "_PRM_U12M"), 4).alias(colName + "_CRECIMIENTO_1M")
    ])
dfMatrizVarTransaccionPosMacrogiroMont = dfMatrizVarTransaccionPosMacrogiroMont.select(original_cols)

# LOOP 2: Procesar TKT
original_cols = dfMatrizVarTransaccionPosMacrogiroMont.columns
for colName in col_names_tkt:
    original_cols.extend([...])  # Mismo patrÃ³n
dfMatrizVarTransaccionPosMacrogiroMont = dfMatrizVarTransaccionPosMacrogiroMont.select(original_cols)

# LOOP 3: Procesar CANTIDAD
original_cols = dfMatrizVarTransaccionPosMacrogiroMont.columns
for colName in col_names_cant:
    original_cols.extend([...])  # Mismo patrÃ³n
dfMatrizVarTransaccionPosMacrogiroMont = dfMatrizVarTransaccionPosMacrogiroMont.select(original_cols)
```

**EDV Version (1 loop consolidado)**
```python
# Preparar TODAS las columnas de una vez
all_new_cols = []

# Procesar MONTO
for colName in col_names_monto:
    all_new_cols.extend([
        func.round(col(colName + "_PRM_U6M")/col(colName + "_PRM_U12M"), 4).alias(colName + "_CRECIMIENTO_6M"),
        func.round(col(colName + "_PRM_U3M")/col(colName + "_PRM_U12M"), 4).alias(colName + "_CRECIMIENTO_3M"),
        func.round(col(colName + "_U1M")/col(colName + "_PRM_U12M"), 4).alias(colName + "_CRECIMIENTO_1M")
    ])

# Procesar TKT
for colName in col_names_tkt:
    all_new_cols.extend([...])

# Procesar CANTIDAD
for colName in col_names_cant:
    all_new_cols.extend([...])

# UNA SOLA transformaciÃ³n
dfMatrizVarTransaccionPosMacrogiroMont = dfMatrizVarTransaccionPosMacrogiroMont.select(
    dfMatrizVarTransaccionPosMacrogiroMont.columns + all_new_cols
)
```

**âš¡ Mejora:**
- DDV: 3 transformaciones Spark â†’ 3 etapas de ejecuciÃ³n
- EDV: 1 transformaciÃ³n Spark â†’ 1 etapa de ejecuciÃ³n
- **ReducciÃ³n de overhead:** ~66% menos stages

---

#### **OPTIMIZACIÃ“N 4: Reparticionamiento Pre-Escritura**

**DDV Version**
```python
def main():
    # ...
    dfMatrizVarTransaccionPosMacrogiro = logicaPostAgrupacionInformacionMesAnalisis(
        dfMatrizVarTransaccionPosMacrogiro
    ).coalesce(160)  # â† Coalesce arbitrario

    write_delta(dfMatrizVarTransaccionPosMacrogiro, VAL_DESTINO_NAME, CONS_PARTITION_DELTA_NAME)
```

**EDV Version**
```python
def main():
    # ...
    dfMatrizVarTransaccionPosMacrogiro = logicaPostAgrupacionInformacionMesAnalisis(
        dfMatrizVarTransaccionPosMacrogiro
    )
    # Eliminado coalesce(160) de densidad no controlada

    # Repartition basado en columna de particiÃ³n
    input_df = input_df.repartition(CONS_PARTITION_DELTA_NAME)  # â† Repartition por CODMES

    write_delta(input_df, VAL_DESTINO_NAME, CONS_PARTITION_DELTA_NAME)
```

**ğŸ¯ Ventajas:**
- `coalesce(160)`: Reduce particiones pero no redistribuye datos (puede crear skew)
- `repartition(CODMES)`: Distribuye equitativamente por particiÃ³n fÃ­sica
- **Resultado:** Archivos balanceados por mes, lecturas futuras mÃ¡s rÃ¡pidas

---

### 5. SETUP Y DEPENDENCIAS

#### **DDV Version**
```python
# No tiene setup inicial
# Asume dependencias pre-instaladas
```

#### **EDV Version (Databricks Notebook Features)**
```python
# COMMAND ----------
dbutils.library.restartPython()

# COMMAND ----------
!pip3 install --trusted-host 10.79.236.20 \
    https://10.79.236.20:443/artifactory/LHCL.Pypi.Snapshot/lhcl/bcp_coe_data_cleaner/1.0.1/bcp_coe_data_cleaner-1.0.1-py3-none-any.whl

# COMMAND ----------
dbutils.widgets.removeAll()  # Limpia widgets previos
```

**ğŸ“Œ Beneficios:**
- âœ… Aislamiento de ambiente (restart Python)
- âœ… InstalaciÃ³n explÃ­cita de dependencias (reproducibilidad)
- âœ… Limpieza de widgets (evita conflictos de parÃ¡metros)

---

## ğŸ”¬ ANÃLISIS DE FUNCIONES

### FunciÃ³n: `extraccionInformacion12Meses()`

**LÃ­neas:**
- DDV: 118-243 (126 lÃ­neas)
- EDV: 164-288 (125 lÃ­neas)

**Diferencias:** IDÃ‰NTICA en lÃ³gica, solo difiere en:
- Lectura desde `PRM_ESQUEMA_TABLA` (ambas versiones)
- EDV usa views (`_v`), DDV usa tablas directas

---

### FunciÃ³n: `agruparInformacionMesAnalisis()`

**LÃ­neas:**
- DDV: 244-602 (359 lÃ­neas)
- EDV: 289-652 (364 lÃ­neas)

**Diferencias CRÃTICAS:**

1. **MaterializaciÃ³n de datos intermedios**
   - DDV: Write/Read a disco (lÃ­neas 457-461)
   - EDV: Cache en memoria (lÃ­neas 483-485)

2. **NÃºmero de lÃ­neas:** +5 lÃ­neas en EDV por comentarios de optimizaciÃ³n

---

### FunciÃ³n: `logicaPostAgrupacionInformacionMesAnalisis()`

**LÃ­neas:**
- DDV: 603-2496 (1,894 lÃ­neas)
- EDV: 653-2550 (1,898 lÃ­neas)

**Diferencias CRÃTICAS:**

1. **Storage Level** (lÃ­neas ~1800)
   - DDV: `MEMORY_ONLY_2`
   - EDV: `MEMORY_AND_DISK_2` + materializaciÃ³n explÃ­cita

2. **ConsolidaciÃ³n de loops** (lÃ­neas ~2200-2400)
   - DDV: 3 transformaciones separadas
   - EDV: 1 transformaciÃ³n consolidada

3. **EliminaciÃ³n de coalesce** (lÃ­neas ~2470)
   - DDV: `.coalesce(160)` en mÃºltiples lugares
   - EDV: Eliminado, usa repartition en main()

---

### FunciÃ³n: `main()`

**LÃ­neas:**
- DDV: 2497-2554 (58 lÃ­neas)
- EDV: 2551-2608 (58 lÃ­neas)

**Diferencias:**

1. **Reparticionamiento pre-escritura**
   - DDV: No tiene
   - EDV: `input_df = input_df.repartition(CONS_PARTITION_DELTA_NAME)`

---

## ğŸ“ˆ TABLA COMPARATIVA DE RENDIMIENTO ESPERADO

| MÃ©trica | DDV | EDV | Mejora |
|---------|-----|-----|--------|
| **Shuffle Partitions** | 1000 | 200 | 5x menos overhead |
| **MaterializaciÃ³n intermedia** | Disco | Memoria (cache) | 10-100x mÃ¡s rÃ¡pido |
| **Storage Level** | MEMORY_ONLY_2 | MEMORY_AND_DISK_2 | MÃ¡s resiliente |
| **Transformaciones** | 3 stages | 1 stage | 3x menos overhead |
| **Particionamiento final** | Coalesce(160) | Repartition(CODMES) | Mejor balance |
| **AQE** | Deshabilitado | Habilitado | Ajuste dinÃ¡mico |
| **Broadcast Join** | Manual | AutomÃ¡tico (64MB) | Joins mÃ¡s rÃ¡pidos |
| **Delta Optimization** | Manual | AutomÃ¡tico | Menos archivos |

**âš¡ Mejora de rendimiento estimada:** **2-5x mÃ¡s rÃ¡pido** en tiempo total de ejecuciÃ³n

---

## ğŸ—ï¸ ARQUITECTURA DE DATOS

### DDV (Desarrollo)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  catalog_lhcl_desa_bcp              â”‚
â”‚  â””â”€ bcp_ddv_matrizvariables         â”‚
â”‚     â”œâ”€ HM_CONCEPTOTRANSACCIONPOS    â”‚  â† READ
â”‚     â””â”€ HM_MATRIZTRANSACCIONPOSMACRO â”‚  â† WRITE (mismo esquema)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### EDV (ProducciÃ³n ExploraciÃ³n)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  catalog_lhcl_prod_bcp              â”‚      â”‚  catalog_lhcl_prod_bcp_expl      â”‚
â”‚  â””â”€ bcp_ddv_matrizvariables_v       â”‚      â”‚  â””â”€ bcp_edv_trdata_012           â”‚
â”‚     â”œâ”€ HM_CONCEPTOTRANSACCIONPOS    â”‚      â”‚     â””â”€ HM_MATRIZTRANSACCIONPOS   â”‚
â”‚     â”‚  (view)                        â”‚â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â†’    MACROGIRO_RUBEN_2      â”‚
â”‚     â””â”€ mm_parametrogrupoconcepto    â”‚ READ â”‚        (tabla EDV)               â”‚
â”‚        (view)                        â”‚      â”‚                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                         â†‘ WRITE
```

**ğŸ”‘ Ventajas arquitectura EDV:**
- âœ… SeparaciÃ³n de ambientes (lectura vs escritura)
- âœ… Views DDV proporcionan capa de abstracciÃ³n
- âœ… Schema EDV dedicado para exploraciÃ³n/producciÃ³n
- âœ… No contamina desarrollo (DDV intacto)

---

## ğŸš€ TABLA DE NOMBRES Y VALORES

### Variables de ConfiguraciÃ³n EspecÃ­ficas

| Variable | DDV Value | EDV Value | PropÃ³sito |
|----------|-----------|-----------|-----------|
| `CONS_CONTAINER_NAME` | `abfss://lhcldata@` | `abfss://bcp-edv-trdata-012@` | Container ADLS Gen2 |
| `PRM_STORAGE_ACCOUNT_DDV` | `adlscu1lhclbackd03` | `adlscu1lhclbackp05` | Storage Account |
| `PRM_ESQUEMA_TABLA_DDV` | `bcp_ddv_matrizvariables` | `bcp_ddv_matrizvariables_v` | Schema DDV (con views) |
| `PRM_CATALOG_NAME` | `catalog_lhcl_desa_bcp` | `catalog_lhcl_prod_bcp` | Catalog principal |
| `PRM_CATALOG_NAME_EDV` | âŒ N/A | `catalog_lhcl_prod_bcp_expl` | Catalog EDV (nuevo) |
| `PRM_ESQUEMA_TABLA_EDV` | âŒ N/A | `bcp_edv_trdata_012` | Schema EDV (nuevo) |
| `PRM_TABLA_PRIMERATRANSPUESTA` | `hm_conceptotransaccionpos` | `hm_conceptotransaccionpos` | Tabla fuente |
| `PRM_TABLE_NAME` | `HM_MATRIZTRANSACCIONPOSMACROGIRO` | `HM_MATRIZTRANSACCIONPOSMACROGIRO_RUBEN_2` | Tabla destino |
| `PRM_RUTA_ADLS_TABLES` | `desa/bcp/ddv/analytics/matrizvariables` | `data/RUBEN/DEUDA_TECNICA/matrizvariables` | Ruta ADLS |

### Variables Derivadas

| Variable | DDV | EDV |
|----------|-----|-----|
| `PRM_ESQUEMA_TABLA` (lectura) | `catalog_lhcl_desa_bcp.bcp_ddv_matrizvariables` | `catalog_lhcl_prod_bcp.bcp_ddv_matrizvariables_v` |
| `PRM_ESQUEMA_TABLA_ESCRITURA` | âŒ No existe | `catalog_lhcl_prod_bcp_expl.bcp_edv_trdata_012` |
| `VAL_DESTINO_NAME` | `catalog_lhcl_desa_bcp.bcp_ddv_matrizvariables.HM_MATRIZTRANSACCIONPOSMACROGIRO` | `catalog_lhcl_prod_bcp_expl.bcp_edv_trdata_012.HM_MATRIZTRANSACCIONPOSMACROGIRO_RUBEN_2` |

---

## âœ… VALIDACIÃ“N DE HOMOLOGÃA

### Funciones IdÃ©nticas (100% match)
- âœ… `write_delta()` - Misma implementaciÃ³n
- âœ… Estructura de queries SQL - IdÃ©nticas
- âœ… LÃ³gica de negocio - IdÃ©ntica
- âœ… CÃ¡lculos de variables - IdÃ©nticos
- âœ… Transformaciones de columnas - IdÃ©nticas

### Diferencias Ãšnicamente en:
- ğŸ”§ ConfiguraciÃ³n de ambiente (DDV vs EDV)
- âš¡ Optimizaciones de Spark (AQE, broadcast, etc.)
- ğŸ’¾ Estrategia de materializaciÃ³n (disco vs cache)
- ğŸ—‚ï¸ Arquitectura de lectura/escritura (mismo schema vs schemas separados)
- ğŸ“ Comentarios y documentaciÃ³n (EDV mÃ¡s detallada)

---

## ğŸ¯ CONCLUSIONES

### 1. Â¿Son homÃ³logos?
**SÃ, 100% homÃ³logos en lÃ³gica de negocio.**

Los scripts implementan exactamente el mismo proceso:
- âœ… Mismas fuentes de datos
- âœ… Mismas transformaciones
- âœ… Mismos cÃ¡lculos de variables
- âœ… Mismo output esperado

### 2. Â¿QuÃ© los diferencia?
**Arquitectura de deployment y optimizaciones de rendimiento.**

| Aspecto | DDV | EDV |
|---------|-----|-----|
| PropÃ³sito | Desarrollo/Testing | ProducciÃ³n (ExploraciÃ³n) |
| Ambiente | Desarrollo (desa) | ProducciÃ³n (prod) |
| OptimizaciÃ³n | BÃ¡sica | Avanzada (AQE, cache, consolidaciÃ³n) |
| Rendimiento | Baseline | 2-5x mÃ¡s rÃ¡pido (estimado) |
| Arquitectura | MonolÃ­tica (mismo schema) | Separada (read DDV, write EDV) |

### 3. Â¿CuÃ¡l usar?
- **DDV:** Para desarrollo, testing, debugging
- **EDV:** Para producciÃ³n, exploraciÃ³n, cargas regulares

### 4. Recomendaciones
1. âœ… Mantener DDV como base de desarrollo
2. âœ… Usar EDV para producciÃ³n (mejor rendimiento)
3. âœ… Sincronizar cambios de lÃ³gica entre ambas versiones
4. âœ… Considerar portar optimizaciones de EDV a DDV
5. âœ… Documentar diferencias de configuraciÃ³n en CLAUDE.md

---

## ğŸ“š REFERENCIAS

- **CLAUDE.md:** GuÃ­a de conversiÃ³n DDV â†’ EDV
- **Spark AQE Docs:** https://spark.apache.org/docs/latest/sql-performance-tuning.html#adaptive-query-execution
- **Delta Lake Optimization:** https://docs.databricks.com/delta/optimizations/index.html

---

**Fecha de anÃ¡lisis:** 2025-10-04
**Analista:** Claude Code
**VersiÃ³n:** 1.0
